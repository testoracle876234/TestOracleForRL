{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import Test_Shy.Util\n",
    "import Env_shy\n",
    "import numpy as np\n",
    "from gymnasium.wrappers import TimeLimit\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import bug_lib\n",
    "import stable_baselines3 as sb3\n",
    "import os\n",
    "import gymnasium as gym\n",
    "import imageio\n",
    "import bug_lib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bug_lib\n",
    "bug_lib.cover_then_inject_bugs([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_gif(model, output_path, num_steps=10000, fps=20):\n",
    "    images = []\n",
    "    obs = model.env.reset()\n",
    "    \n",
    "    # 渲染初始状态\n",
    "    img = model.env.render(mode='rgb_array')\n",
    "    images.append(img)\n",
    "    \n",
    "    for i in range(num_steps):\n",
    "        action, _ = model.predict(obs)\n",
    "        obs, _, done, _ = model.env.step(action)\n",
    "        \n",
    "        img = model.env.render(mode='rgb_array')\n",
    "        images.append(img)\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    model.env.close()  # 关闭环境\n",
    "    \n",
    "    imageio.mimsave(output_path, images, fps=fps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_and_save_matrix(n, m, trial_no, num_arrays):\n",
    "#     folder_path = f'./saved_array/2by2/{trial_no}/'\n",
    "#     os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "#     for i in range(num_arrays):\n",
    "#         A, B = Test_Shy.Util.generate_state_transition_matix(n, m)\n",
    "#         np.save(f'./saved_array/2by2/{trial_no}/array_A_{i}.npy'.format(trial_no=trial_no, i=i), A)\n",
    "#         np.save(f'./saved_array/2by2/{trial_no}/array_B_{i}.npy'.format(trial_no=trial_no, i=i), B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A2C: [24, 28, 29, 31, 32, 40, 41, 44, 45, 46, 47, 48, 49, 50, 27, 26,6, 10, 52]\n",
    "# PPO: [54, 28, 55, 56, 57, 58, 44, 45, 46, 47, 48, 49, 50, 27, 26, 6, 10, 52, 59, 60, 61, 62]\n",
    "# TD3: [53, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75]\n",
    "\n",
    "\n",
    "# TD3: [53, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75]\n",
    "bug_no = 70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from typing import Any, ClassVar, Dict, List, Optional, Tuple, Type, TypeVar, Union\n",
      "\n",
      "import numpy as np\n",
      "import torch as th\n",
      "from gymnasium import spaces\n",
      "from torch.nn import functional as F\n",
      "\n",
      "from stable_baselines3.common.buffers import ReplayBuffer\n",
      "from stable_baselines3.common.noise import ActionNoise\n",
      "from stable_baselines3.common.off_policy_algorithm import OffPolicyAlgorithm\n",
      "from stable_baselines3.common.policies import BasePolicy, ContinuousCritic\n",
      "from stable_baselines3.common.type_aliases import GymEnv, MaybeCallback, Schedule\n",
      "from stable_baselines3.common.utils import get_parameters_by_name, polyak_update\n",
      "from stable_baselines3.td3.policies import Actor, CnnPolicy, MlpPolicy, MultiInputPolicy, TD3Policy\n",
      "\n",
      "SelfTD3 = TypeVar(\"SelfTD3\", bound=\"TD3\")\n",
      "\n",
      "\n",
      "class TD3(OffPolicyAlgorithm):\n",
      "    \"\"\"\n",
      "    Twin Delayed DDPG (TD3)\n",
      "    Addressing Function Approximation Error in Actor-Critic Methods.\n",
      "\n",
      "    Original implementation: https://github.com/sfujim/TD3\n",
      "    Paper: https://arxiv.org/abs/1802.09477\n",
      "    Introduction to TD3: https://spinningup.openai.com/en/latest/algorithms/td3.html\n",
      "\n",
      "    :param policy: The policy model to use (MlpPolicy, CnnPolicy, ...)\n",
      "    :param env: The environment to learn from (if registered in Gym, can be str)\n",
      "    :param learning_rate: learning rate for adam optimizer,\n",
      "        the same learning rate will be used for all networks (Q-Values, Actor and Value function)\n",
      "        it can be a function of the current progress remaining (from 1 to 0)\n",
      "    :param buffer_size: size of the replay buffer\n",
      "    :param learning_starts: how many steps of the model to collect transitions for before learning starts\n",
      "    :param batch_size: Minibatch size for each gradient update\n",
      "    :param tau: the soft update coefficient (\"Polyak update\", between 0 and 1)\n",
      "    :param gamma: the discount factor\n",
      "    :param train_freq: Update the model every ``train_freq`` steps. Alternatively pass a tuple of frequency and unit\n",
      "        like ``(5, \"step\")`` or ``(2, \"episode\")``.\n",
      "    :param gradient_steps: How many gradient steps to do after each rollout (see ``train_freq``)\n",
      "        Set to ``-1`` means to do as many gradient steps as steps done in the environment\n",
      "        during the rollout.\n",
      "    :param action_noise: the action noise type (None by default), this can help\n",
      "        for hard exploration problem. Cf common.noise for the different action noise type.\n",
      "    :param replay_buffer_class: Replay buffer class to use (for instance ``HerReplayBuffer``).\n",
      "        If ``None``, it will be automatically selected.\n",
      "    :param replay_buffer_kwargs: Keyword arguments to pass to the replay buffer on creation.\n",
      "    :param optimize_memory_usage: Enable a memory efficient variant of the replay buffer\n",
      "        at a cost of more complexity.\n",
      "        See https://github.com/DLR-RM/stable-baselines3/issues/37#issuecomment-637501195\n",
      "    :param policy_delay: Policy and target networks will only be updated once every policy_delay steps\n",
      "        per training steps. The Q values will be updated policy_delay more often (update every training step).\n",
      "    :param target_policy_noise: Standard deviation of Gaussian noise added to target policy\n",
      "        (smoothing noise)\n",
      "    :param target_noise_clip: Limit for absolute value of target policy smoothing noise.\n",
      "    :param stats_window_size: Window size for the rollout logging, specifying the number of episodes to average\n",
      "        the reported success rate, mean episode length, and mean reward over\n",
      "    :param tensorboard_log: the log location for tensorboard (if None, no logging)\n",
      "    :param policy_kwargs: additional arguments to be passed to the policy on creation\n",
      "    :param verbose: Verbosity level: 0 for no output, 1 for info messages (such as device or wrappers used), 2 for\n",
      "        debug messages\n",
      "    :param seed: Seed for the pseudo random generators\n",
      "    :param device: Device (cpu, cuda, ...) on which the code should be run.\n",
      "        Setting it to auto, the code will be run on the GPU if possible.\n",
      "    :param _init_setup_model: Whether or not to build the network at the creation of the instance\n",
      "    \"\"\"\n",
      "\n",
      "    policy_aliases: ClassVar[Dict[str, Type[BasePolicy]]] = {\n",
      "        \"MlpPolicy\": MlpPolicy,\n",
      "        \"CnnPolicy\": CnnPolicy,\n",
      "        \"MultiInputPolicy\": MultiInputPolicy,\n",
      "    }\n",
      "    policy: TD3Policy\n",
      "    actor: Actor\n",
      "    actor_target: Actor\n",
      "    critic: ContinuousCritic\n",
      "    critic_target: ContinuousCritic\n",
      "\n",
      "    def __init__(\n",
      "        self,\n",
      "        policy: Union[str, Type[TD3Policy]],\n",
      "        env: Union[GymEnv, str],\n",
      "        learning_rate: Union[float, Schedule] = 1e-3,\n",
      "        buffer_size: int = 1_000_000,  # 1e6\n",
      "        learning_starts: int = 100,\n",
      "        batch_size: int = 256,\n",
      "        tau: float = 0.005,\n",
      "        gamma: float = 0.99,\n",
      "        train_freq: Union[int, Tuple[int, str]] = 1,\n",
      "        gradient_steps: int = 1,\n",
      "        action_noise: Optional[ActionNoise] = None,\n",
      "        replay_buffer_class: Optional[Type[ReplayBuffer]] = None,\n",
      "        replay_buffer_kwargs: Optional[Dict[str, Any]] = None,\n",
      "        optimize_memory_usage: bool = False,\n",
      "        policy_delay: int = 2,\n",
      "        target_policy_noise: float = 0.2,\n",
      "        target_noise_clip: float = 0.5,\n",
      "        stats_window_size: int = 100,\n",
      "        tensorboard_log: Optional[str] = None,\n",
      "        policy_kwargs: Optional[Dict[str, Any]] = None,\n",
      "        verbose: int = 0,\n",
      "        seed: Optional[int] = None,\n",
      "        device: Union[th.device, str] = \"auto\",\n",
      "        _init_setup_model: bool = True,\n",
      "    ):\n",
      "        super().__init__(\n",
      "            policy,\n",
      "            env,\n",
      "            learning_rate,\n",
      "            buffer_size,\n",
      "            learning_starts,\n",
      "            batch_size,\n",
      "            tau,\n",
      "            gamma,\n",
      "            train_freq,\n",
      "            gradient_steps,\n",
      "            action_noise=action_noise,\n",
      "            replay_buffer_class=replay_buffer_class,\n",
      "            replay_buffer_kwargs=replay_buffer_kwargs,\n",
      "            policy_kwargs=policy_kwargs,\n",
      "            stats_window_size=stats_window_size,\n",
      "            tensorboard_log=tensorboard_log,\n",
      "            verbose=verbose,\n",
      "            device=device,\n",
      "            seed=seed,\n",
      "            sde_support=False,\n",
      "            optimize_memory_usage=optimize_memory_usage,\n",
      "            supported_action_spaces=(spaces.Box,),\n",
      "            support_multi_env=True,\n",
      "        )\n",
      "\n",
      "        self.policy_delay = policy_delay\n",
      "        self.target_noise_clip = target_noise_clip\n",
      "        self.target_policy_noise = target_policy_noise\n",
      "\n",
      "        if _init_setup_model:\n",
      "            self._setup_model()\n",
      "\n",
      "    def _setup_model(self) -> None:\n",
      "        super()._setup_model()\n",
      "        self._create_aliases()\n",
      "        # Running mean and running var\n",
      "        self.actor_batch_norm_stats = get_parameters_by_name(self.actor, [\"running_\"])\n",
      "        self.critic_batch_norm_stats = get_parameters_by_name(self.critic, [\"running_\"])\n",
      "        self.actor_batch_norm_stats_target = get_parameters_by_name(self.actor_target, [\"running_\"])\n",
      "        self.critic_batch_norm_stats_target = get_parameters_by_name(self.critic_target, [\"running_\"])\n",
      "\n",
      "    def _create_aliases(self) -> None:\n",
      "        self.actor = self.policy.actor\n",
      "        self.actor_target = self.policy.actor_target\n",
      "        self.critic = self.policy.critic\n",
      "        self.critic_target = self.policy.critic_target\n",
      "\n",
      "    def train(self, gradient_steps: int, batch_size: int = 100) -> None:\n",
      "        # Switch to train mode (this affects batch norm / dropout)\n",
      "        self.policy.set_training_mode(True)\n",
      "\n",
      "        # Update learning rate according to lr schedule\n",
      "        self._update_learning_rate([self.actor.optimizer, self.critic.optimizer])\n",
      "\n",
      "        actor_losses, critic_losses = [], []\n",
      "        for _ in range(gradient_steps):\n",
      "            self._n_updates += 1\n",
      "            # Sample replay buffer\n",
      "            replay_data = self.replay_buffer.sample(batch_size, env=self._vec_normalize_env)  # type: ignore[union-attr]\n",
      "\n",
      "            with th.no_grad():\n",
      "                # Select action according to policy and add clipped noise\n",
      "                noise = replay_data.actions.clone().data.normal_(0, self.target_policy_noise)\n",
      "                noise = noise.clamp(-self.target_noise_clip, self.target_noise_clip)\n",
      "                next_actions = (self.actor_target(replay_data.next_observations) + noise).clamp(-1, 1)\n",
      "\n",
      "                # Compute the next Q-values: min over all critics targets\n",
      "                next_q_values = th.cat(self.critic_target(replay_data.next_observations, next_actions), dim=1)\n",
      "                next_q_values, _ = th.min(next_q_values, dim=1, keepdim=True)\n",
      "                target_q_values = replay_data.rewards + (1 - replay_data.dones) * self.gamma * next_q_values\n",
      "\n",
      "            # Get current Q-values estimates for each critic network\n",
      "            current_q_values = self.critic(replay_data.observations, replay_data.actions)\n",
      "\n",
      "            # Compute critic loss\n",
      "            critic_loss = sum(F.mse_loss(current_q, target_q_values) for current_q in current_q_values)\n",
      "            assert isinstance(critic_loss, th.Tensor)\n",
      "            critic_losses.append(critic_loss.item())\n",
      "\n",
      "            # Optimize the critics\n",
      "            self.critic.optimizer.zero_grad()\n",
      "            critic_loss.backward()\n",
      "            self.critic.optimizer.step()\n",
      "\n",
      "            # Delayed policy updates\n",
      "            if self._n_updates % self.policy_delay == 0:\n",
      "                # Compute actor loss\n",
      "                actor_loss = -self.critic.q1_forward(replay_data.observations, self.actor(replay_data.observations)).mean()\n",
      "                actor_losses.append(actor_loss.item())\n",
      "\n",
      "                # Optimize the actor\n",
      "                self.actor.optimizer.zero_grad()\n",
      "                actor_loss.backward()\n",
      "                self.actor.optimizer.step()\n",
      "\n",
      "                polyak_update(self.critic.parameters(), self.critic_target.parameters(), self.tau)\n",
      "                polyak_update(self.actor.parameters(), self.actor_target.parameters(), self.tau)\n",
      "                # Copy running stats, see GH issue #996\n",
      "                polyak_update(self.critic_batch_norm_stats, self.critic_batch_norm_stats_target, 1.0)\n",
      "                polyak_update(self.actor_batch_norm_stats, self.actor_batch_norm_stats_target, 1.0)\n",
      "\n",
      "        self.logger.record(\"train/n_updates\", self._n_updates, exclude=\"tensorboard\")\n",
      "        if len(actor_losses) > 0:\n",
      "            self.logger.record(\"train/actor_loss\", np.mean(actor_losses))\n",
      "        self.logger.record(\"train/critic_loss\", np.mean(critic_losses))\n",
      "\n",
      "    def learn(\n",
      "        self: SelfTD3,\n",
      "        total_timesteps: int,\n",
      "        callback: MaybeCallback = None,\n",
      "        log_interval: int = 4,\n",
      "        tb_log_name: str = \"TD3\",\n",
      "        reset_num_timesteps: bool = True,\n",
      "        progress_bar: bool = False,\n",
      "    ) -> SelfTD3:\n",
      "        return super().learn(\n",
      "            total_timesteps=total_timesteps,\n",
      "            callback=callback,\n",
      "            log_interval=log_interval,\n",
      "            tb_log_name=tb_log_name,\n",
      "            reset_num_timesteps=reset_num_timesteps,\n",
      "            progress_bar=progress_bar,\n",
      "        )\n",
      "\n",
      "    def _excluded_save_params(self) -> List[str]:\n",
      "        return super()._excluded_save_params() + [\"actor\", \"critic\", \"actor_target\", \"critic_target\"]  # noqa: RUF005\n",
      "\n",
      "    def _get_torch_save_params(self) -> Tuple[List[str], List[str]]:\n",
      "        state_dicts = [\"policy\", \"actor.optimizer\", \"critic.optimizer\"]\n",
      "        return state_dicts, []\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import bug_lib\n",
    "\n",
    "\n",
    "# A2Clib    [24, 28, 29, 31, 32, 40, 41, 44, 45, 46, 47, 48, 49, 50, 27, 26, 6, 10, 52]\n",
    "# A2C clean [7, 12, 15, 18, 20, 33, 35, 37, 39, 42, 51, 54, 56, 58, 60, 62, 64, 66, 68]\n",
    "\n",
    "\n",
    "# PPO:           [54, 28, 55, 56, 57, 58, 44, 45, 46, 47, 48, 49, 50, 27, 26, 6, 10, 52, 59, 60, 61, 62]\n",
    "# PPO bugfree no:[37, 12, 81, 93, 76, 42, 19, 33, 67, 85, 71, 24, 39, 8, 16, 2, 90, 53, 69, 74, 31, 22]\n",
    "\n",
    "if bug_no != 'free':\n",
    "    bug_lib.cover_then_inject_bugs([bug_no])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to ./TensorBoard_Logs/human_oracle/env_Pendulum/alg_td3/version_no_70/learning_steps_10000\\TD3_1\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 200       |\n",
      "|    ep_rew_mean     | -1.51e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 4         |\n",
      "|    fps             | 71        |\n",
      "|    time_elapsed    | 11        |\n",
      "|    total_timesteps | 800       |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 17.7      |\n",
      "|    critic_loss     | 0.0245    |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 699       |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 200       |\n",
      "|    ep_rew_mean     | -1.46e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 8         |\n",
      "|    fps             | 67        |\n",
      "|    time_elapsed    | 23        |\n",
      "|    total_timesteps | 1600      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 29.6      |\n",
      "|    critic_loss     | 0.0351    |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 1499      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 200       |\n",
      "|    ep_rew_mean     | -1.43e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 12        |\n",
      "|    fps             | 66        |\n",
      "|    time_elapsed    | 36        |\n",
      "|    total_timesteps | 2400      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 42.2      |\n",
      "|    critic_loss     | 0.0717    |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 2299      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 200       |\n",
      "|    ep_rew_mean     | -1.37e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 16        |\n",
      "|    fps             | 65        |\n",
      "|    time_elapsed    | 48        |\n",
      "|    total_timesteps | 3200      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 50.3      |\n",
      "|    critic_loss     | 0.167     |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 3099      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 200       |\n",
      "|    ep_rew_mean     | -1.32e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 20        |\n",
      "|    fps             | 65        |\n",
      "|    time_elapsed    | 61        |\n",
      "|    total_timesteps | 4000      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 63        |\n",
      "|    critic_loss     | 0.224     |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 3899      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 200       |\n",
      "|    ep_rew_mean     | -1.21e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 24        |\n",
      "|    fps             | 64        |\n",
      "|    time_elapsed    | 73        |\n",
      "|    total_timesteps | 4800      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 65.5      |\n",
      "|    critic_loss     | 0.435     |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 4699      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 200       |\n",
      "|    ep_rew_mean     | -1.09e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 28        |\n",
      "|    fps             | 64        |\n",
      "|    time_elapsed    | 86        |\n",
      "|    total_timesteps | 5600      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 67.3      |\n",
      "|    critic_loss     | 0.255     |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 5499      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 200       |\n",
      "|    ep_rew_mean     | -1.01e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 32        |\n",
      "|    fps             | 64        |\n",
      "|    time_elapsed    | 99        |\n",
      "|    total_timesteps | 6400      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 70        |\n",
      "|    critic_loss     | 0.869     |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 6299      |\n",
      "----------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -961     |\n",
      "| time/              |          |\n",
      "|    episodes        | 36       |\n",
      "|    fps             | 63       |\n",
      "|    time_elapsed    | 113      |\n",
      "|    total_timesteps | 7200     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 76.1     |\n",
      "|    critic_loss     | 0.85     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 7099     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -884     |\n",
      "| time/              |          |\n",
      "|    episodes        | 40       |\n",
      "|    fps             | 62       |\n",
      "|    time_elapsed    | 127      |\n",
      "|    total_timesteps | 8000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 71       |\n",
      "|    critic_loss     | 1.78     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 7899     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -815     |\n",
      "| time/              |          |\n",
      "|    episodes        | 44       |\n",
      "|    fps             | 62       |\n",
      "|    time_elapsed    | 140      |\n",
      "|    total_timesteps | 8800     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 69.8     |\n",
      "|    critic_loss     | 2.23     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 8699     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -764     |\n",
      "| time/              |          |\n",
      "|    episodes        | 48       |\n",
      "|    fps             | 62       |\n",
      "|    time_elapsed    | 153      |\n",
      "|    total_timesteps | 9600     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 66.1     |\n",
      "|    critic_loss     | 1.67     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 9499     |\n",
      "---------------------------------\n",
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to ./TensorBoard_Logs/human_oracle/env_Pendulum/alg_td3/version_no_70/learning_steps_20000\\TD3_1\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 200       |\n",
      "|    ep_rew_mean     | -1.58e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 4         |\n",
      "|    fps             | 72        |\n",
      "|    time_elapsed    | 11        |\n",
      "|    total_timesteps | 800       |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 17.9      |\n",
      "|    critic_loss     | 0.0589    |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 699       |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 200       |\n",
      "|    ep_rew_mean     | -1.49e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 8         |\n",
      "|    fps             | 67        |\n",
      "|    time_elapsed    | 23        |\n",
      "|    total_timesteps | 1600      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 29.6      |\n",
      "|    critic_loss     | 0.0538    |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 1499      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 200       |\n",
      "|    ep_rew_mean     | -1.45e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 12        |\n",
      "|    fps             | 66        |\n",
      "|    time_elapsed    | 36        |\n",
      "|    total_timesteps | 2400      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 40.2      |\n",
      "|    critic_loss     | 0.221     |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 2299      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 200       |\n",
      "|    ep_rew_mean     | -1.38e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 16        |\n",
      "|    fps             | 65        |\n",
      "|    time_elapsed    | 48        |\n",
      "|    total_timesteps | 3200      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 52.9      |\n",
      "|    critic_loss     | 0.127     |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 3099      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 200       |\n",
      "|    ep_rew_mean     | -1.37e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 20        |\n",
      "|    fps             | 65        |\n",
      "|    time_elapsed    | 61        |\n",
      "|    total_timesteps | 4000      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 63        |\n",
      "|    critic_loss     | 0.216     |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 3899      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 200       |\n",
      "|    ep_rew_mean     | -1.23e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 24        |\n",
      "|    fps             | 64        |\n",
      "|    time_elapsed    | 74        |\n",
      "|    total_timesteps | 4800      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 64.3      |\n",
      "|    critic_loss     | 0.491     |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 4699      |\n",
      "----------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -1.1e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 28       |\n",
      "|    fps             | 63       |\n",
      "|    time_elapsed    | 87       |\n",
      "|    total_timesteps | 5600     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 71.5     |\n",
      "|    critic_loss     | 0.373    |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 5499     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -982     |\n",
      "| time/              |          |\n",
      "|    episodes        | 32       |\n",
      "|    fps             | 63       |\n",
      "|    time_elapsed    | 100      |\n",
      "|    total_timesteps | 6400     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 69       |\n",
      "|    critic_loss     | 0.388    |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 6299     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -890     |\n",
      "| time/              |          |\n",
      "|    episodes        | 36       |\n",
      "|    fps             | 63       |\n",
      "|    time_elapsed    | 113      |\n",
      "|    total_timesteps | 7200     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 64.9     |\n",
      "|    critic_loss     | 0.52     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 7099     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -825     |\n",
      "| time/              |          |\n",
      "|    episodes        | 40       |\n",
      "|    fps             | 63       |\n",
      "|    time_elapsed    | 126      |\n",
      "|    total_timesteps | 8000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 59.7     |\n",
      "|    critic_loss     | 0.53     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 7899     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -761     |\n",
      "| time/              |          |\n",
      "|    episodes        | 44       |\n",
      "|    fps             | 63       |\n",
      "|    time_elapsed    | 138      |\n",
      "|    total_timesteps | 8800     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 69.7     |\n",
      "|    critic_loss     | 0.582    |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 8699     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -705     |\n",
      "| time/              |          |\n",
      "|    episodes        | 48       |\n",
      "|    fps             | 63       |\n",
      "|    time_elapsed    | 151      |\n",
      "|    total_timesteps | 9600     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 61.8     |\n",
      "|    critic_loss     | 1.5      |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 9499     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -661     |\n",
      "| time/              |          |\n",
      "|    episodes        | 52       |\n",
      "|    fps             | 63       |\n",
      "|    time_elapsed    | 164      |\n",
      "|    total_timesteps | 10400    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 72.8     |\n",
      "|    critic_loss     | 1.68     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 10299    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -624     |\n",
      "| time/              |          |\n",
      "|    episodes        | 56       |\n",
      "|    fps             | 63       |\n",
      "|    time_elapsed    | 176      |\n",
      "|    total_timesteps | 11200    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 68.5     |\n",
      "|    critic_loss     | 1.11     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 11099    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -591     |\n",
      "| time/              |          |\n",
      "|    episodes        | 60       |\n",
      "|    fps             | 63       |\n",
      "|    time_elapsed    | 189      |\n",
      "|    total_timesteps | 12000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 58.3     |\n",
      "|    critic_loss     | 0.806    |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 11899    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -562     |\n",
      "| time/              |          |\n",
      "|    episodes        | 64       |\n",
      "|    fps             | 63       |\n",
      "|    time_elapsed    | 202      |\n",
      "|    total_timesteps | 12800    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 56.9     |\n",
      "|    critic_loss     | 1.15     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 12699    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -541     |\n",
      "| time/              |          |\n",
      "|    episodes        | 68       |\n",
      "|    fps             | 63       |\n",
      "|    time_elapsed    | 214      |\n",
      "|    total_timesteps | 13600    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 66.9     |\n",
      "|    critic_loss     | 1.68     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 13499    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -520     |\n",
      "| time/              |          |\n",
      "|    episodes        | 72       |\n",
      "|    fps             | 63       |\n",
      "|    time_elapsed    | 227      |\n",
      "|    total_timesteps | 14400    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 66.1     |\n",
      "|    critic_loss     | 2.22     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 14299    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -501     |\n",
      "| time/              |          |\n",
      "|    episodes        | 76       |\n",
      "|    fps             | 63       |\n",
      "|    time_elapsed    | 239      |\n",
      "|    total_timesteps | 15200    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 63.7     |\n",
      "|    critic_loss     | 3.87     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 15099    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -482     |\n",
      "| time/              |          |\n",
      "|    episodes        | 80       |\n",
      "|    fps             | 63       |\n",
      "|    time_elapsed    | 252      |\n",
      "|    total_timesteps | 16000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 52.4     |\n",
      "|    critic_loss     | 1.76     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 15899    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -468     |\n",
      "| time/              |          |\n",
      "|    episodes        | 84       |\n",
      "|    fps             | 63       |\n",
      "|    time_elapsed    | 265      |\n",
      "|    total_timesteps | 16800    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 57.7     |\n",
      "|    critic_loss     | 3.52     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 16699    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -454     |\n",
      "| time/              |          |\n",
      "|    episodes        | 88       |\n",
      "|    fps             | 63       |\n",
      "|    time_elapsed    | 277      |\n",
      "|    total_timesteps | 17600    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 53.8     |\n",
      "|    critic_loss     | 2.44     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 17499    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -442     |\n",
      "| time/              |          |\n",
      "|    episodes        | 92       |\n",
      "|    fps             | 63       |\n",
      "|    time_elapsed    | 290      |\n",
      "|    total_timesteps | 18400    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 59.4     |\n",
      "|    critic_loss     | 3.98     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 18299    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -432     |\n",
      "| time/              |          |\n",
      "|    episodes        | 96       |\n",
      "|    fps             | 63       |\n",
      "|    time_elapsed    | 302      |\n",
      "|    total_timesteps | 19200    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 49.1     |\n",
      "|    critic_loss     | 3.15     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 19099    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -425     |\n",
      "| time/              |          |\n",
      "|    episodes        | 100      |\n",
      "|    fps             | 63       |\n",
      "|    time_elapsed    | 315      |\n",
      "|    total_timesteps | 20000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 62.9     |\n",
      "|    critic_loss     | 5.59     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 19899    |\n",
      "---------------------------------\n",
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to ./TensorBoard_Logs/human_oracle/env_Pendulum/alg_td3/version_no_70/learning_steps_30000\\TD3_1\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 200       |\n",
      "|    ep_rew_mean     | -1.49e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 4         |\n",
      "|    fps             | 67        |\n",
      "|    time_elapsed    | 11        |\n",
      "|    total_timesteps | 800       |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 15.8      |\n",
      "|    critic_loss     | 0.0496    |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 699       |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 200       |\n",
      "|    ep_rew_mean     | -1.51e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 8         |\n",
      "|    fps             | 65        |\n",
      "|    time_elapsed    | 24        |\n",
      "|    total_timesteps | 1600      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 28.5      |\n",
      "|    critic_loss     | 0.0538    |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 1499      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 200       |\n",
      "|    ep_rew_mean     | -1.47e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 12        |\n",
      "|    fps             | 64        |\n",
      "|    time_elapsed    | 37        |\n",
      "|    total_timesteps | 2400      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 40.2      |\n",
      "|    critic_loss     | 0.0677    |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 2299      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 200       |\n",
      "|    ep_rew_mean     | -1.45e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 16        |\n",
      "|    fps             | 64        |\n",
      "|    time_elapsed    | 49        |\n",
      "|    total_timesteps | 3200      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 52.8      |\n",
      "|    critic_loss     | 0.13      |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 3099      |\n",
      "----------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -1.4e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 20       |\n",
      "|    fps             | 64       |\n",
      "|    time_elapsed    | 62       |\n",
      "|    total_timesteps | 4000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 60.8     |\n",
      "|    critic_loss     | 0.262    |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 3899     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 200       |\n",
      "|    ep_rew_mean     | -1.26e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 24        |\n",
      "|    fps             | 64        |\n",
      "|    time_elapsed    | 74        |\n",
      "|    total_timesteps | 4800      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 68.9      |\n",
      "|    critic_loss     | 0.457     |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 4699      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 200       |\n",
      "|    ep_rew_mean     | -1.15e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 28        |\n",
      "|    fps             | 64        |\n",
      "|    time_elapsed    | 87        |\n",
      "|    total_timesteps | 5600      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 71.5      |\n",
      "|    critic_loss     | 0.349     |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 5499      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 200       |\n",
      "|    ep_rew_mean     | -1.03e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 32        |\n",
      "|    fps             | 63        |\n",
      "|    time_elapsed    | 100       |\n",
      "|    total_timesteps | 6400      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 71.1      |\n",
      "|    critic_loss     | 0.702     |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 6299      |\n",
      "----------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -936     |\n",
      "| time/              |          |\n",
      "|    episodes        | 36       |\n",
      "|    fps             | 63       |\n",
      "|    time_elapsed    | 112      |\n",
      "|    total_timesteps | 7200     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 75.4     |\n",
      "|    critic_loss     | 0.728    |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 7099     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -858     |\n",
      "| time/              |          |\n",
      "|    episodes        | 40       |\n",
      "|    fps             | 63       |\n",
      "|    time_elapsed    | 125      |\n",
      "|    total_timesteps | 8000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 68.2     |\n",
      "|    critic_loss     | 0.874    |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 7899     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -799     |\n",
      "| time/              |          |\n",
      "|    episodes        | 44       |\n",
      "|    fps             | 63       |\n",
      "|    time_elapsed    | 138      |\n",
      "|    total_timesteps | 8800     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 73.4     |\n",
      "|    critic_loss     | 0.574    |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 8699     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -740     |\n",
      "| time/              |          |\n",
      "|    episodes        | 48       |\n",
      "|    fps             | 63       |\n",
      "|    time_elapsed    | 150      |\n",
      "|    total_timesteps | 9600     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 67.1     |\n",
      "|    critic_loss     | 1.17     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 9499     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -696     |\n",
      "| time/              |          |\n",
      "|    episodes        | 52       |\n",
      "|    fps             | 63       |\n",
      "|    time_elapsed    | 163      |\n",
      "|    total_timesteps | 10400    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 69.3     |\n",
      "|    critic_loss     | 0.784    |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 10299    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -663     |\n",
      "| time/              |          |\n",
      "|    episodes        | 56       |\n",
      "|    fps             | 63       |\n",
      "|    time_elapsed    | 176      |\n",
      "|    total_timesteps | 11200    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 65.8     |\n",
      "|    critic_loss     | 2.09     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 11099    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -629     |\n",
      "| time/              |          |\n",
      "|    episodes        | 60       |\n",
      "|    fps             | 63       |\n",
      "|    time_elapsed    | 188      |\n",
      "|    total_timesteps | 12000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 68.7     |\n",
      "|    critic_loss     | 2.27     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 11899    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -597     |\n",
      "| time/              |          |\n",
      "|    episodes        | 64       |\n",
      "|    fps             | 63       |\n",
      "|    time_elapsed    | 201      |\n",
      "|    total_timesteps | 12800    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 63.3     |\n",
      "|    critic_loss     | 2.67     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 12699    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -571     |\n",
      "| time/              |          |\n",
      "|    episodes        | 68       |\n",
      "|    fps             | 63       |\n",
      "|    time_elapsed    | 213      |\n",
      "|    total_timesteps | 13600    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 60.5     |\n",
      "|    critic_loss     | 7.37     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 13499    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -546     |\n",
      "| time/              |          |\n",
      "|    episodes        | 72       |\n",
      "|    fps             | 63       |\n",
      "|    time_elapsed    | 226      |\n",
      "|    total_timesteps | 14400    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 72.6     |\n",
      "|    critic_loss     | 2.61     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 14299    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -524     |\n",
      "| time/              |          |\n",
      "|    episodes        | 76       |\n",
      "|    fps             | 63       |\n",
      "|    time_elapsed    | 238      |\n",
      "|    total_timesteps | 15200    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 69       |\n",
      "|    critic_loss     | 2.78     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 15099    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -507     |\n",
      "| time/              |          |\n",
      "|    episodes        | 80       |\n",
      "|    fps             | 63       |\n",
      "|    time_elapsed    | 251      |\n",
      "|    total_timesteps | 16000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 64       |\n",
      "|    critic_loss     | 3.13     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 15899    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -490     |\n",
      "| time/              |          |\n",
      "|    episodes        | 84       |\n",
      "|    fps             | 63       |\n",
      "|    time_elapsed    | 264      |\n",
      "|    total_timesteps | 16800    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 71.1     |\n",
      "|    critic_loss     | 2.23     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 16699    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -478     |\n",
      "| time/              |          |\n",
      "|    episodes        | 88       |\n",
      "|    fps             | 63       |\n",
      "|    time_elapsed    | 276      |\n",
      "|    total_timesteps | 17600    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 64.8     |\n",
      "|    critic_loss     | 3.5      |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 17499    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -467     |\n",
      "| time/              |          |\n",
      "|    episodes        | 92       |\n",
      "|    fps             | 63       |\n",
      "|    time_elapsed    | 289      |\n",
      "|    total_timesteps | 18400    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 52.6     |\n",
      "|    critic_loss     | 4.33     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 18299    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -454     |\n",
      "| time/              |          |\n",
      "|    episodes        | 96       |\n",
      "|    fps             | 63       |\n",
      "|    time_elapsed    | 301      |\n",
      "|    total_timesteps | 19200    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 48.4     |\n",
      "|    critic_loss     | 1.77     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 19099    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -439     |\n",
      "| time/              |          |\n",
      "|    episodes        | 100      |\n",
      "|    fps             | 63       |\n",
      "|    time_elapsed    | 314      |\n",
      "|    total_timesteps | 20000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 50.4     |\n",
      "|    critic_loss     | 6.18     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 19899    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -384     |\n",
      "| time/              |          |\n",
      "|    episodes        | 104      |\n",
      "|    fps             | 63       |\n",
      "|    time_elapsed    | 326      |\n",
      "|    total_timesteps | 20800    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 60.3     |\n",
      "|    critic_loss     | 2.4      |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 20699    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -332     |\n",
      "| time/              |          |\n",
      "|    episodes        | 108      |\n",
      "|    fps             | 63       |\n",
      "|    time_elapsed    | 339      |\n",
      "|    total_timesteps | 21600    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 49.8     |\n",
      "|    critic_loss     | 4.91     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 21499    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -283     |\n",
      "| time/              |          |\n",
      "|    episodes        | 112      |\n",
      "|    fps             | 63       |\n",
      "|    time_elapsed    | 351      |\n",
      "|    total_timesteps | 22400    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 46.1     |\n",
      "|    critic_loss     | 4.46     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 22299    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -232     |\n",
      "| time/              |          |\n",
      "|    episodes        | 116      |\n",
      "|    fps             | 63       |\n",
      "|    time_elapsed    | 364      |\n",
      "|    total_timesteps | 23200    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 59.8     |\n",
      "|    critic_loss     | 7.09     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 23099    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -190     |\n",
      "| time/              |          |\n",
      "|    episodes        | 120      |\n",
      "|    fps             | 63       |\n",
      "|    time_elapsed    | 377      |\n",
      "|    total_timesteps | 24000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 43.7     |\n",
      "|    critic_loss     | 16.3     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 23899    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -171     |\n",
      "| time/              |          |\n",
      "|    episodes        | 124      |\n",
      "|    fps             | 63       |\n",
      "|    time_elapsed    | 389      |\n",
      "|    total_timesteps | 24800    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 46.1     |\n",
      "|    critic_loss     | 13       |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 24699    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -159     |\n",
      "| time/              |          |\n",
      "|    episodes        | 128      |\n",
      "|    fps             | 63       |\n",
      "|    time_elapsed    | 402      |\n",
      "|    total_timesteps | 25600    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 55.7     |\n",
      "|    critic_loss     | 4.66     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 25499    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -153     |\n",
      "| time/              |          |\n",
      "|    episodes        | 132      |\n",
      "|    fps             | 63       |\n",
      "|    time_elapsed    | 414      |\n",
      "|    total_timesteps | 26400    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 41.1     |\n",
      "|    critic_loss     | 4.97     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 26299    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -154     |\n",
      "| time/              |          |\n",
      "|    episodes        | 136      |\n",
      "|    fps             | 63       |\n",
      "|    time_elapsed    | 427      |\n",
      "|    total_timesteps | 27200    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 48.1     |\n",
      "|    critic_loss     | 4.54     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 27099    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -151     |\n",
      "| time/              |          |\n",
      "|    episodes        | 140      |\n",
      "|    fps             | 63       |\n",
      "|    time_elapsed    | 439      |\n",
      "|    total_timesteps | 28000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 54.5     |\n",
      "|    critic_loss     | 2.16     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 27899    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -150     |\n",
      "| time/              |          |\n",
      "|    episodes        | 144      |\n",
      "|    fps             | 63       |\n",
      "|    time_elapsed    | 452      |\n",
      "|    total_timesteps | 28800    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 53.3     |\n",
      "|    critic_loss     | 2.23     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 28699    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -151     |\n",
      "| time/              |          |\n",
      "|    episodes        | 148      |\n",
      "|    fps             | 63       |\n",
      "|    time_elapsed    | 464      |\n",
      "|    total_timesteps | 29600    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 47.1     |\n",
      "|    critic_loss     | 5.98     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 29499    |\n",
      "---------------------------------\n",
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to ./TensorBoard_Logs/human_oracle/env_MountainCarContinuous/alg_td3/version_no_70/learning_steps_10000\\TD3_1\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 999      |\n",
      "|    ep_rew_mean     | -0.971   |\n",
      "| time/              |          |\n",
      "|    episodes        | 4        |\n",
      "|    fps             | 65       |\n",
      "|    time_elapsed    | 60       |\n",
      "|    total_timesteps | 3996     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.0429   |\n",
      "|    critic_loss     | 4.19e-05 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 3895     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 999      |\n",
      "|    ep_rew_mean     | -0.5     |\n",
      "| time/              |          |\n",
      "|    episodes        | 8        |\n",
      "|    fps             | 64       |\n",
      "|    time_elapsed    | 123      |\n",
      "|    total_timesteps | 7992     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.0856   |\n",
      "|    critic_loss     | 4.57e-05 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 7891     |\n",
      "---------------------------------\n",
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to ./TensorBoard_Logs/human_oracle/env_MountainCarContinuous/alg_td3/version_no_70/learning_steps_20000\\TD3_1\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 999      |\n",
      "|    ep_rew_mean     | -0.854   |\n",
      "| time/              |          |\n",
      "|    episodes        | 4        |\n",
      "|    fps             | 65       |\n",
      "|    time_elapsed    | 60       |\n",
      "|    total_timesteps | 3996     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.208    |\n",
      "|    critic_loss     | 4.62e-05 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 3895     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 999      |\n",
      "|    ep_rew_mean     | -0.432   |\n",
      "| time/              |          |\n",
      "|    episodes        | 8        |\n",
      "|    fps             | 65       |\n",
      "|    time_elapsed    | 122      |\n",
      "|    total_timesteps | 7992     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.23     |\n",
      "|    critic_loss     | 5.87e-05 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 7891     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 999      |\n",
      "|    ep_rew_mean     | -0.292   |\n",
      "| time/              |          |\n",
      "|    episodes        | 12       |\n",
      "|    fps             | 64       |\n",
      "|    time_elapsed    | 185      |\n",
      "|    total_timesteps | 11988    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.243    |\n",
      "|    critic_loss     | 5.56e-05 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 11887    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 999      |\n",
      "|    ep_rew_mean     | -0.223   |\n",
      "| time/              |          |\n",
      "|    episodes        | 16       |\n",
      "|    fps             | 64       |\n",
      "|    time_elapsed    | 247      |\n",
      "|    total_timesteps | 15984    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.255    |\n",
      "|    critic_loss     | 6.3e-05  |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 15883    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 999      |\n",
      "|    ep_rew_mean     | -0.181   |\n",
      "| time/              |          |\n",
      "|    episodes        | 20       |\n",
      "|    fps             | 64       |\n",
      "|    time_elapsed    | 310      |\n",
      "|    total_timesteps | 19980    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.267    |\n",
      "|    critic_loss     | 6.3e-05  |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 19879    |\n",
      "---------------------------------\n",
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to ./TensorBoard_Logs/human_oracle/env_MountainCarContinuous/alg_td3/version_no_70/learning_steps_30000\\TD3_1\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 999      |\n",
      "|    ep_rew_mean     | -0.964   |\n",
      "| time/              |          |\n",
      "|    episodes        | 4        |\n",
      "|    fps             | 65       |\n",
      "|    time_elapsed    | 61       |\n",
      "|    total_timesteps | 3996     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.067    |\n",
      "|    critic_loss     | 6.57e-05 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 3895     |\n",
      "---------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 69\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m env \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPendulum\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMountainCarContinuous\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_trails):\n\u001b[1;32m---> 69\u001b[0m         \u001b[43mrun_one_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malgorithm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malgorithm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial_no\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbug_no\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbug_no\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[2], line 58\u001b[0m, in \u001b[0;36mrun_one_trial\u001b[1;34m(env, algorithm, trial_no, bug_no)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m algorithm \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtd3\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m     57\u001b[0m     model \u001b[38;5;241m=\u001b[39m sb3\u001b[38;5;241m.\u001b[39mTD3(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMlpPolicy\u001b[39m\u001b[38;5;124m\"\u001b[39m, env, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, tensorboard_log\u001b[38;5;241m=\u001b[39mfile_path_log)\n\u001b[1;32m---> 58\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlearning_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     60\u001b[0m model\u001b[38;5;241m.\u001b[39msave(file_path_log \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     61\u001b[0m generate_gif(model, file_path_log \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_training.gif\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\OneDrive\\Desktop\\SB3Testing\\RL-Oracle-Lyapunov\\stable_baselines3\\td3\\td3.py:222\u001b[0m, in \u001b[0;36mTD3.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfTD3,\n\u001b[0;32m    215\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    220\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    221\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfTD3:\n\u001b[1;32m--> 222\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    223\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    224\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    226\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    228\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    229\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\OneDrive\\Desktop\\SB3Testing\\RL-Oracle-Lyapunov\\stable_baselines3\\common\\off_policy_algorithm.py:347\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    345\u001b[0m         \u001b[38;5;66;03m# Special case when the user passes `gradient_steps=0`\u001b[39;00m\n\u001b[0;32m    346\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m gradient_steps \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 347\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgradient_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    349\u001b[0m callback\u001b[38;5;241m.\u001b[39mon_training_end()\n\u001b[0;32m    351\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\OneDrive\\Desktop\\SB3Testing\\RL-Oracle-Lyapunov\\stable_baselines3\\td3\\td3.py:188\u001b[0m, in \u001b[0;36mTD3.train\u001b[1;34m(self, gradient_steps, batch_size)\u001b[0m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;66;03m# Optimize the critics\u001b[39;00m\n\u001b[0;32m    187\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritic\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m--> 188\u001b[0m \u001b[43mcritic_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritic\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    191\u001b[0m \u001b[38;5;66;03m# Delayed policy updates\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\iansy\\.conda\\envs\\RL_Oracle_Lyapunov\\lib\\site-packages\\torch\\_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    524\u001b[0m     )\n\u001b[1;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\iansy\\.conda\\envs\\RL_Oracle_Lyapunov\\lib\\site-packages\\torch\\autograd\\__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\iansy\\.conda\\envs\\RL_Oracle_Lyapunov\\lib\\site-packages\\torch\\autograd\\graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    745\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    746\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import Test_Shy.Util\n",
    "import Env_shy\n",
    "import numpy as np\n",
    "from gymnasium.wrappers import TimeLimit\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import bug_lib\n",
    "import stable_baselines3 as sb3\n",
    "import os\n",
    "import gymnasium as gym\n",
    "import imageio\n",
    "\n",
    "\n",
    "def generate_gif(model, output_path, num_steps=10000, fps=20):\n",
    "    images = []\n",
    "    obs = model.env.reset()\n",
    "    \n",
    "    # 渲染初始状态\n",
    "    img = model.env.render(mode='rgb_array')\n",
    "    images.append(img)\n",
    "    \n",
    "    for i in range(num_steps):\n",
    "        action, _ = model.predict(obs)\n",
    "        obs, _, done, _ = model.env.step(action)\n",
    "        \n",
    "        img = model.env.render(mode='rgb_array')\n",
    "        images.append(img)\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    model.env.close()  # 关闭环境\n",
    "    \n",
    "    imageio.mimsave(output_path, images, fps=fps)\n",
    "\n",
    "def run_one_trial(env, algorithm, trial_no, bug_no):\n",
    "    learning_steps = (trial_no + 1) * 10000\n",
    "    # learning_steps = 500\n",
    "\n",
    "    #bug free log multiple steps\n",
    "    file_path_log = './TensorBoard_Logs/human_oracle/env_{env}/alg_{alg}/version_no_{bug_no}/learning_steps_{learning_steps}'.format(env=env, trial_no=trial_no, bug_no=bug_no, alg=algorithm, learning_steps=learning_steps)\n",
    "\n",
    "    # for i in range(1):\n",
    "    if env == 'MountainCarContinuous':\n",
    "        env = gym.make('MountainCarContinuous-v0', render_mode=\"rgb_array\")\n",
    "    elif env =='Pendulum':\n",
    "        env = gym.make('Pendulum-v1', g=9.81, render_mode=\"rgb_array\")\n",
    "\n",
    "    if algorithm == 'ppo':\n",
    "        model = sb3.PPO(\"MlpPolicy\", env, verbose=1, tensorboard_log=file_path_log)\n",
    "        model.learn(total_timesteps=learning_steps)\n",
    "    elif algorithm == 'a2c':\n",
    "        model = sb3.A2C(\"MlpPolicy\", env, verbose=1, tensorboard_log=file_path_log)\n",
    "        model.learn(total_timesteps=learning_steps)\n",
    "    elif algorithm == 'td3':\n",
    "        model = sb3.TD3(\"MlpPolicy\", env, verbose=1, tensorboard_log=file_path_log)\n",
    "        model.learn(total_timesteps=learning_steps)\n",
    "\n",
    "    model.save(file_path_log + 'model')\n",
    "    generate_gif(model, file_path_log + 'after_training.gif')\n",
    "\n",
    "num_trails = 3\n",
    "algorithm = 'td3'\n",
    "# env = 'Pendulum'\n",
    "# env = 'MountainCarContinuous'\n",
    "for env in ['Pendulum', 'MountainCarContinuous']:\n",
    "    for i in range(num_trails):\n",
    "        run_one_trial(env=env, algorithm=algorithm, trial_no=i, bug_no=bug_no)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir TensorBoard_Logs/human_oracle/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --version"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SB_Testing_new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
